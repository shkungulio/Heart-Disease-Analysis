---
title: "Forecasting Heart Disease Risks: Random Forest"
output:
  flexdashboard::flex_dashboard:
    logo: "../image/logo.gif"
    theme:
      version: 4
      bootswatch: cosmo
      base_font:
        google: Montserrat
      code_font:
        google: Inconsolata
    orientation: columns
    vertical_layout: fill
---

<style>
body {
  font-size: 16px;
}
h4 {
  font-size: 24px;
}
h1, h2, h3 {
  font-size: 28px;
}
</style>


```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(flexdashboard)
#Install thematic and un-comment for themed static plots (i.e., ggplot2)
#thematic::thematic_rmd(font = "auto")

# Install necessary packages if they are not already installed.
if (!requireNamespace("RCurl", quietly = TRUE)) {
  install.packages("RCurl")
}
if (!requireNamespace("tidyverse", quietly = TRUE)) {
  install.packages("tidyverse")
}
if (!requireNamespace("patchwork", quietly = TRUE)) {
  install.packages("patchwork")
}
if (!requireNamespace("corrplot", quietly = TRUE)) {
  install.packages("corrplot")
}

# Load the required libraries
library(RCurl)
library(tidyverse)
library(patchwork)
library(corrplot)
library(GGally)
```

Column {data-width=600 .tabset}
-----------------------------------------------------------------------

### BUSINESS UNDERSTANDING

```{r}

```



### DATA UNDERSTANDING

```{r}

```



### DATA PREPARATION

```{r}

```



### MODELING

```{r}

```



### EVALUATION

```{r}

```



### DEPLOYMENT

```{r}

```




Column {data-width=250 .tabset}
-----------------------------------------------------------------------

### PROBLEM STATEMENT

```{r}

```

####

```{r echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("../image/heart.png")
```

### CONCLUSION

```{r}

```

Column {data-width=150 .tabset}
-----------------------------------------------------------------------

### Random Forest

**Overview:** 

- An ensemble of decision trees where each tree is trained on a random subset of the data.
- The final prediction is the majority vote (classification) or average (regression) of all trees.

**Advantages:**  

- Reduces overfitting compared to individual decision trees.
- Handles missing values and maintains accuracy for a large portion of missing data.
- Works well for both classification and regression.

**Disadvantages:**

- Can be computationally intensive with many trees.
- Less interpretable than a single decision tree.
- May not perform well on very high-dimensional sparse data.
