---
title: "Forecasting Heart Disease Risks: k-Nearest Neighbor (KNN)"
output:
  flexdashboard::flex_dashboard:
    logo: "../image/logo.gif"
    theme:
      version: 4
      bootswatch: cosmo
      base_font:
        google: Montserrat
      code_font:
        google: Inconsolata
    orientation: columns
    vertical_layout: fill
---

<style>
body {
  font-size: 16px;
}
h4 {
  font-size: 24px;
}
h1, h2, h3 {
  font-size: 28px;
}
</style>


```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(flexdashboard)
#Install thematic and un-comment for themed static plots (i.e., ggplot2)
#thematic::thematic_rmd(font = "auto")

# Install necessary packages if they are not already installed.
if (!requireNamespace("RCurl", quietly = TRUE)) {
  install.packages("RCurl")
}
if (!requireNamespace("tidyverse", quietly = TRUE)) {
  install.packages("tidyverse")
}
if (!requireNamespace("patchwork", quietly = TRUE)) {
  install.packages("patchwork")
}
if (!requireNamespace("corrplot", quietly = TRUE)) {
  install.packages("corrplot")
}

# Load the required libraries
library(RCurl)
library(tidyverse)
library(patchwork)
library(corrplot)
library(GGally)
```

Column {data-width=600 .tabset}
-----------------------------------------------------------------------

### BUSINESS UNDERSTANDING

```{r}

```



### DATA UNDERSTANDING

```{r}

```



### DATA PREPARATION

```{r}

```



### MODELING

```{r}

```



### EVALUATION

```{r}

```



### DEPLOYMENT

```{r}

```




Column {data-width=250 .tabset}
-----------------------------------------------------------------------

### PROBLEM STATEMENT

```{r}

```

####

```{r echo=FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("../image/heart.png")
```

### CONCLUSION

```{r}

```

Column {data-width=150 .tabset}
-----------------------------------------------------------------------

### k-Nearest Neighbor

**Overview:**

- A simple, instance-based learning method.
- Classifies a new point based on the majority label of its **k** closest neighbors.

**Advantages:**

- Simple and intuitive.
- No training phase (lazy learning).
- Naturally handles multi-class problems.

**Disadvantages:**

- Slow prediction time as dataset size grows.
- Sensitive to irrelevant features and the choice of distance metric.
- Struggles with high-dimensional data (curse of dimensionality).
